---
title: "PML-Course Project"
author: ""
date: "15/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

We usually quantify our exercises rather than rate it by quality. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har 
This is a project on the Practical Machine Learning course on Coursera where we need to predict the manner in which exercise is performed on a test data set using a sample data called the train data. 

## Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Getting and Cleaning Data

We now load the packages useful for this project.

```{r}
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
```

We now load the data and name the training set as pmltrain and the testing data set as pmltest.
```{r}
pmltrain<-read.csv("D:/R/Practical Machine Learning/pml-training.csv", header=TRUE, na.strings = "NA")
pmltest<-read.csv("D:/R/Practical Machine Learning/pml-testing.csv", header=TRUE)
dim(pmltrain)
dim(pmltest)
```

Looking into the data, we can notice that "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window", "roll_belt", "pitch_belt", "yaw_belt" are variables which are irrelevant to our project in prediting the model. So we remove these variables along with this we also remove those variables which has high number of NA's.

Any columns removed on the training datat set must also be removed on the test data set otherwise while predicting the model, prediction will not be proper.

```{r}
#Removing irrelevant variables
pmltrainnew<- pmltrain[,-c(1:10)]
pmltestnew<- pmltest[,-c(1:10)]
dim(pmltrainnew)
dim(pmltestnew)

#Identifying rows with NA
na_count <-sapply(pmltrainnew, function(y) sum(length(which(is.na(y)))))
table(na_count)
```

From the table we can notice that there are columns with either zero missing values or 19216 missing values (or NA). We apply preprocessing in each column if the missing values are less than 75%.

```{r}
#Let us now check if the missing values are less than 75%
unique(colSums(is.na(pmltrainnew))/nrow(pmltrainnew))
```

There are around 67 columns which has 98% more missing values than non-empty entries. Hence, removing those columns is more advisable than applying preprocessing. 

```{r}
missing<-sapply(pmltrainnew, function(y) sum(length(which(is.na(y)))))
pmltrainnew<-pmltrainnew[,missing==FALSE]
pmltestnew<-pmltestnew[,missing==FALSE]
dim(pmltrainnew)
dim(pmltestnew)
```

Using str function on the data set we see that there are a few near zero columns. So we use the nearzerovar function of the caret package to remove those columns.

```{r}
NZV<-nearZeroVar(pmltrainnew)
pmltrainnew<-pmltrainnew[,-NZV]
pmltestnew<-pmltestnew[,-NZV]
```

## Partitioning for cross-validation

A better fit of the mdoel can be obtained while applying cross-validation. We use the train set to partition into training and validation set
```{r}

inTrain<- createDataPartition(y=pmltrainnew$classe, p=0.7,list=FALSE)
pmltraining<- pmltrainnew[inTrain,]
pmlvalidation<-pmltrainnew[-inTrain,]
```


## Fitting a model

To find the right method to predict the model we utilise methods like- random forest, gbm, lda and combination of methods to predict the model and pick the method with highest accuracy or least error. 

Since the data set is huge and calculations can take alot of time, I have utilized parallel computing method while predicting the model. This reduces time drastically and helps is fast computation.

```{r}

#Parallel computing command due to large data set
cluster<-makeCluster(detectCores()-1)
registerDoParallel(cluster)
fitControl<-trainControl(method="cv",
                         number=5,
                         allowParallel = TRUE)

#Fitting a model with rf method and predicting values
modrf<-train(classe~.,data=pmltraining,method="rf", trControl=fitControl)
predrf<-predict(modrf, pmlvalidation)

#Fitting a model with gbm method and predicting values
modgbm<-train(classe~.,data=pmltraining, method="gbm", verbose=FALSE)
predgbm<-predict(modgbm, pmlvalidation)


#Fitting a model with lda method and predicting values
modlda<-train(classe~.,data=pmltraining, method="lda")
predlda<-predict(modlda, pmlvalidation)

#Creating data frames for combined models
predrgDF<-data.frame(predrf,predgbm,classe=pmlvalidation$classe)
predglDF<-data.frame(predgbm,predlda,classe=pmlvalidation$classe)
predrlDF<-data.frame(predrf,predlda,classe=pmlvalidation$classe)
predrglDF<-data.frame(predrf,predgbm,predlda,classe=pmlvalidation$classe)

#Combining models and predicting the variables
combrg<-train(classe~.,data=predrgDF)
combgl<-train(classe~.,data=predglDF)
combrl<-train(classe~.,data=predrlDF)
combrgl<-train(classe~.,data=predrglDF)

#stop the cluster used for parallel computation
stopCluster(cluster)
registerDoSEQ()

combpredrg<-predict(combrg,data=predrgDF)
combpredgl<-predict(combgl,data=predglDF)
combpredrl<-predict(combrl,data=predrlDF)
combpredrgl<-predict(combrgl,data=predrglDF)

#Calculating Confusion Matrix for each model to calculate accuracy
confusionMatrix(predrf, pmlvalidation$classe)$overall[1]
confusionMatrix(predgbm, pmlvalidation$classe)$overall[1]
confusionMatrix(predlda, pmlvalidation$classe)$overall[1]

confusionMatrix(combpredrg, pmlvalidation$classe)$overall[1]
confusionMatrix(combpredgl, pmlvalidation$classe)$overall[1]
confusionMatrix(combpredrl, pmlvalidation$classe)$overall[1]
confusionMatrix(combpredrgl, pmlvalidation$classe)$overall[1]

```

From the above models we can see that models using Random Forest method has the highest accuracy which is 98.9%. 

```{r}
modrf$finalModel
plot(modrf)
```

The out of bag error is 1.14%. Hence we move forward using the Random Forest method.

## Prediction on the test data

Predicting using the Random Forest model:
```{r}
Testpred<-predict(modrf,newdata = pmltest)
Testpred
```

Result above predicts how well (in terms of quality) the exercise was performed by individuals in the test data.